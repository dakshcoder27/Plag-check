{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ae1a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f67a63d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_code(code):\n",
    "    code = re.sub(r'//.*|/\\*[\\s\\S]*?\\*/|#.*', '', code)  # Remove comments\n",
    "    code = re.sub(r'\\s+', ' ', code)  # Normalize whitespace\n",
    "    return code.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ed3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_code_files(folder_path):\n",
    "    files = []\n",
    "    contents = []\n",
    "    code=[]\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith(\".py\") or filename.endswith(\".cpp\") or filename.endswith(\".java\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                files.append(filename)\n",
    "                contents.append(preprocess_code(file.read()))\n",
    "                code.append(file.read())\n",
    "    return files, contents,code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd1914d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_dag(code_folder, threshold=0.2):\n",
    "    files, code_snippets,code = load_code_files(code_folder)\n",
    "    \n",
    "    # Vectorize using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(code_snippets)\n",
    "    \n",
    "    # Cosine similarity matrix\n",
    "    similarity = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # ðŸ“Š Build DAG\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(files)\n",
    "\n",
    "    cheaters = []  # Store suspicious file pairs with their code\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        for j in range(i+1, len(files)):  # Only i -> j to prevent cycles\n",
    "            sim_score = similarity[i][j]\n",
    "            if sim_score > threshold:\n",
    "                G.add_edge(files[i], files[j], weight=sim_score)\n",
    "                cheaters.append((\n",
    "                    files[i], files[j], sim_score,\n",
    "                    code[i],  # preprocessed content of file i\n",
    "                    code[j]   # preprocessed content of file j\n",
    "                ))\n",
    "    \n",
    "    return G, similarity, files, cheaters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1e4723c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkadance Algo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m dag, sim_matrix, filenames,cheaters \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_similarity_dag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ðŸ“ˆ Visualize\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[58], line 6\u001b[0m, in \u001b[0;36mcompute_similarity_dag\u001b[1;34m(code_folder, threshold)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Vectorize using TF-IDF\u001b[39;00m\n\u001b[0;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m----> 6\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_snippets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Cosine similarity matrix\u001b[39;00m\n\u001b[0;32m      9\u001b[0m similarity \u001b[38;5;241m=\u001b[39m cosine_similarity(tfidf_matrix)\n",
      "File \u001b[1;32mc:\\Users\\deepak jain\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2090\u001b[0m )\n\u001b[1;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\deepak jain\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deepak jain\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1350\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;66;03m# We intentionally don't call the transform method to make\u001b[39;00m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;66;03m# fit_transform overridable without unwanted side effects in\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;66;03m# TfidfVectorizer.\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable over raw text documents expected, string object received.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1352\u001b[0m     )\n\u001b[0;32m   1354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_ngram_range()\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "folder=r\"kadance Algo\"\n",
    "dag, sim_matrix, filenames,cheaters = compute_similarity_dag(folder)\n",
    "\n",
    "# ðŸ“ˆ Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pos = nx.spring_layout(dag)\n",
    "nx.draw(dag, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=2000)\n",
    "edge_labels = nx.get_edge_attributes(dag, 'weight')\n",
    "nx.draw_networkx_edge_labels(dag, pos, edge_labels={k: f\"{v:.2f}\" for k, v in edge_labels.items()})\n",
    "plt.title(\"Code Similarity DAG\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e21c6f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - s1.cpp and s10.cpp â†’ Similarity: 0.29\n",
      " - s1.cpp and s5.cpp â†’ Similarity: 0.32\n",
      " - s1.cpp and s7.cpp â†’ Similarity: 0.28\n",
      " - s1.cpp and s8.cpp â†’ Similarity: 0.28\n",
      " - s1.cpp and s9.cpp â†’ Similarity: 0.28\n",
      " - s10.cpp and s3.cpp â†’ Similarity: 0.39\n",
      " - s10.cpp and s5.cpp â†’ Similarity: 0.26\n",
      " - s10.cpp and s7.cpp â†’ Similarity: 0.28\n",
      " - s10.cpp and s8.cpp â†’ Similarity: 0.29\n",
      " - s10.cpp and s9.cpp â†’ Similarity: 0.28\n",
      " - s2.cpp and s6.cpp â†’ Similarity: 0.47\n",
      " - s4.cpp and s7.cpp â†’ Similarity: 0.34\n",
      " - s4.cpp and s8.cpp â†’ Similarity: 0.34\n",
      " - s5.cpp and s7.cpp â†’ Similarity: 0.24\n",
      " - s5.cpp and s8.cpp â†’ Similarity: 0.25\n",
      " - s5.cpp and s9.cpp â†’ Similarity: 0.24\n",
      " - s7.cpp and s8.cpp â†’ Similarity: 0.98\n",
      " - s7.cpp and s9.cpp â†’ Similarity: 0.26\n",
      " - s8.cpp and s9.cpp â†’ Similarity: 0.27\n",
      " - test1.cpp and test2.cpp â†’ Similarity: 0.97\n"
     ]
    }
   ],
   "source": [
    "for f1, f2, score, _, _ in cheaters:\n",
    "    print(f\" - {f1} and {f2} â†’ Similarity: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "316814fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def ast_similarity(code1, code2):\n",
    "    try:\n",
    "        tree1 = ast.parse(code1)\n",
    "        tree2 = ast.parse(code2)\n",
    "    except Exception as e:\n",
    "        return 0.0  # Parsing error â†’ treat as dissimilar\n",
    "    \n",
    "    def count_nodes(node):\n",
    "        return sum(1 for _ in ast.walk(node))\n",
    "    \n",
    "    # Simple similarity metric: intersection / union of node counts\n",
    "    count1 = count_nodes(tree1)\n",
    "    count2 = count_nodes(tree2)\n",
    "    \n",
    "    common = sum(1 for n1, n2 in zip(ast.walk(tree1), ast.walk(tree2)) if type(n1) == type(n2))\n",
    "    union = count1 + count2 - common\n",
    "    \n",
    "    return common / union if union != 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "687fb18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1.cpp <--> s10.cpp | TF-IDF: 0.29 | AST Sim: 0.00\n",
      "s1.cpp <--> s5.cpp | TF-IDF: 0.32 | AST Sim: 0.00\n",
      "s1.cpp <--> s7.cpp | TF-IDF: 0.28 | AST Sim: 0.00\n",
      "s1.cpp <--> s8.cpp | TF-IDF: 0.28 | AST Sim: 0.00\n",
      "s1.cpp <--> s9.cpp | TF-IDF: 0.28 | AST Sim: 0.00\n",
      "s10.cpp <--> s3.cpp | TF-IDF: 0.39 | AST Sim: 0.00\n",
      "s10.cpp <--> s5.cpp | TF-IDF: 0.26 | AST Sim: 0.00\n",
      "s10.cpp <--> s7.cpp | TF-IDF: 0.28 | AST Sim: 0.00\n",
      "s10.cpp <--> s8.cpp | TF-IDF: 0.29 | AST Sim: 0.00\n",
      "s10.cpp <--> s9.cpp | TF-IDF: 0.28 | AST Sim: 0.00\n",
      "s2.cpp <--> s6.cpp | TF-IDF: 0.47 | AST Sim: 0.00\n",
      "s4.cpp <--> s7.cpp | TF-IDF: 0.34 | AST Sim: 0.00\n",
      "s4.cpp <--> s8.cpp | TF-IDF: 0.34 | AST Sim: 0.00\n",
      "s5.cpp <--> s7.cpp | TF-IDF: 0.24 | AST Sim: 0.00\n",
      "s5.cpp <--> s8.cpp | TF-IDF: 0.25 | AST Sim: 0.00\n",
      "s5.cpp <--> s9.cpp | TF-IDF: 0.24 | AST Sim: 0.00\n",
      "s7.cpp <--> s8.cpp | TF-IDF: 0.98 | AST Sim: 0.00\n",
      "s7.cpp <--> s9.cpp | TF-IDF: 0.26 | AST Sim: 0.00\n",
      "s8.cpp <--> s9.cpp | TF-IDF: 0.27 | AST Sim: 0.00\n",
      "test1.cpp <--> test2.cpp | TF-IDF: 0.97 | AST Sim: 0.00\n"
     ]
    }
   ],
   "source": [
    "for file1, file2, tfidf_score, code1, code2 in cheaters:\n",
    "    ast_sim = ast_similarity(code1, code2)\n",
    "    print(f\"{file1} <--> {file2} | TF-IDF: {tfidf_score:.2f} | AST Sim: {ast_sim:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4c023bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ Similar Code Buckets (AST â‰¥ 0.7):\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "buckets = defaultdict(set)\n",
    "\n",
    "for file1, file2, _, code1, code2 in cheaters:\n",
    "    sim = ast_similarity(code1, code2)\n",
    "    if sim > 0.7:\n",
    "        buckets[file1].add(file2)\n",
    "        buckets[file2].add(file1)\n",
    "\n",
    "# Print buckets\n",
    "print(\"\\nðŸ“¦ Similar Code Buckets (AST â‰¥ 0.7):\")\n",
    "for key, group in buckets.items():\n",
    "    print(f\"{key}: {group}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbae447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
